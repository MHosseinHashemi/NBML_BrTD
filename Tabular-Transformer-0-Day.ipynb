{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa313c0c",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429922e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2 \n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152 # for feature extraction\n",
    "from torch.utils.data import DataLoader # for batching data\n",
    "from torch.utils.data import TensorDataset \n",
    "from torchvision.transforms import transforms \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafd47e",
   "metadata": {},
   "source": [
    "# Load the Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f10339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep_features = []\n",
    "# root = 'C:\\\\Users\\\\Eurus\\\\Desktop\\\\Data\\\\images'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# # Load the ResNet-152 model\n",
    "# resnet = resnet152(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1951eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.to(device)\n",
    "# resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916e609",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9cc7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transformation for preprocessing\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize((540, 540)),\n",
    "#     transforms.ToTensor()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ba3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_extractor(root):\n",
    "#     # Iterate over the images\n",
    "#     for idx in tqdm(os.listdir(root)):\n",
    "        \n",
    "#         # Read the image\n",
    "#         path = os.path.join(root, idx)\n",
    "#         img = Image.open(path)\n",
    "        \n",
    "#         # Preprocess the images (Normalize-> add a dummy dimention -> move it to cuda)\n",
    "#         img_tensor = preprocess(img).div(255.0).unsqueeze(0).to(device)\n",
    "        \n",
    "#         # Extract features using ResNet-152\n",
    "#         with torch.no_grad():\n",
    "#             features = resnet(img_tensor)\n",
    "            \n",
    "#         # Flatten the features\n",
    "#         features = features.flatten().cpu().numpy()\n",
    "        \n",
    "#         # Store the features in the dictionary\n",
    "#         if \"T\" in idx:\n",
    "#             Deep_features.append({'Label':'T','Features':features})\n",
    "#         elif \"N\" in idx:\n",
    "#             Deep_features.append({'Label':'N','Features':features})\n",
    "#         else:\n",
    "#             print(\"error!\")\n",
    "            \n",
    "    \n",
    "#     return Deep_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef08e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_features = feature_extractor(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f17d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "# random.shuffle(Train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4345ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cleaning feature titles to make the dataset easy to use\"\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# # Iterate over the list of dictionaries\n",
    "# for idx, d in enumerate(Train_features):\n",
    "#     # Extract the label and features from each dictionary\n",
    "#     label = d['Label']\n",
    "#     features = d['Features']\n",
    "    \n",
    "#     # Create a dictionary for the row data\n",
    "#     row_data = {'Label': label}\n",
    "    \n",
    "#     # Add the features as columns to the row dictionary\n",
    "#     for i, value in enumerate(features):\n",
    "#         column_name = f'F_{i}'\n",
    "#         row_data[column_name] = value\n",
    "    \n",
    "#     # Append the row to the DataFrame\n",
    "#     df = df.append(row_data, ignore_index=True)\n",
    "\n",
    "    \n",
    "# # Now we have df, Delete the residual\n",
    "# del Train_features\n",
    "\n",
    "# # Observe the results\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dc0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Encoding the Labels\"\n",
    "\n",
    "# for row in tqdm(range(len(df))):\n",
    "#     if df.loc[row,'Label'] == \"N\":\n",
    "#         df.loc[row,'Label'] = 0\n",
    "        \n",
    "#     elif df.loc[row,'Label'] == \"T\":\n",
    "#         df.loc[row,'Label'] = 1\n",
    "\n",
    "# # Convert them all to float32        \n",
    "# df = df.astype('float32')\n",
    "# df.info()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52402df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Beware !!!\n",
    "# df.to_csv(\"df_output_cell_10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a8a451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>F_0</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>...</th>\n",
       "      <th>F_990</th>\n",
       "      <th>F_991</th>\n",
       "      <th>F_992</th>\n",
       "      <th>F_993</th>\n",
       "      <th>F_994</th>\n",
       "      <th>F_995</th>\n",
       "      <th>F_996</th>\n",
       "      <th>F_997</th>\n",
       "      <th>F_998</th>\n",
       "      <th>F_999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-3155.3340</td>\n",
       "      <td>2247.9016</td>\n",
       "      <td>-15242.063</td>\n",
       "      <td>2105.8013</td>\n",
       "      <td>4712.2056</td>\n",
       "      <td>26482.980</td>\n",
       "      <td>29321.460</td>\n",
       "      <td>22290.460</td>\n",
       "      <td>12380.1455</td>\n",
       "      <td>...</td>\n",
       "      <td>24386.512</td>\n",
       "      <td>-22758.016</td>\n",
       "      <td>32603.027</td>\n",
       "      <td>-22886.885</td>\n",
       "      <td>22423.560</td>\n",
       "      <td>-25667.980</td>\n",
       "      <td>7625.3670</td>\n",
       "      <td>9067.743</td>\n",
       "      <td>13116.528</td>\n",
       "      <td>300.804350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4456.2400</td>\n",
       "      <td>5013.8184</td>\n",
       "      <td>-19840.195</td>\n",
       "      <td>3260.8843</td>\n",
       "      <td>5470.0503</td>\n",
       "      <td>33030.970</td>\n",
       "      <td>37948.010</td>\n",
       "      <td>27413.734</td>\n",
       "      <td>15669.2110</td>\n",
       "      <td>...</td>\n",
       "      <td>31678.412</td>\n",
       "      <td>-29751.850</td>\n",
       "      <td>42726.203</td>\n",
       "      <td>-28719.370</td>\n",
       "      <td>28016.207</td>\n",
       "      <td>-33457.973</td>\n",
       "      <td>11405.8260</td>\n",
       "      <td>12074.846</td>\n",
       "      <td>16793.018</td>\n",
       "      <td>76.287735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-3624.0283</td>\n",
       "      <td>4096.3154</td>\n",
       "      <td>-17587.424</td>\n",
       "      <td>2470.3438</td>\n",
       "      <td>5148.6680</td>\n",
       "      <td>28676.377</td>\n",
       "      <td>33115.723</td>\n",
       "      <td>23879.389</td>\n",
       "      <td>13839.6810</td>\n",
       "      <td>...</td>\n",
       "      <td>27025.328</td>\n",
       "      <td>-26016.256</td>\n",
       "      <td>36537.504</td>\n",
       "      <td>-24437.940</td>\n",
       "      <td>24452.035</td>\n",
       "      <td>-28386.310</td>\n",
       "      <td>9967.6120</td>\n",
       "      <td>10893.868</td>\n",
       "      <td>14938.218</td>\n",
       "      <td>215.583150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4195.8760</td>\n",
       "      <td>4789.3535</td>\n",
       "      <td>-18938.963</td>\n",
       "      <td>2818.3047</td>\n",
       "      <td>5316.8325</td>\n",
       "      <td>31045.184</td>\n",
       "      <td>36170.170</td>\n",
       "      <td>26332.742</td>\n",
       "      <td>15174.0280</td>\n",
       "      <td>...</td>\n",
       "      <td>29719.297</td>\n",
       "      <td>-28442.473</td>\n",
       "      <td>40481.420</td>\n",
       "      <td>-27291.717</td>\n",
       "      <td>26547.879</td>\n",
       "      <td>-31284.033</td>\n",
       "      <td>10983.9375</td>\n",
       "      <td>11585.030</td>\n",
       "      <td>15812.540</td>\n",
       "      <td>209.395650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5584.1300</td>\n",
       "      <td>6379.3710</td>\n",
       "      <td>-23731.717</td>\n",
       "      <td>3972.9082</td>\n",
       "      <td>6766.4370</td>\n",
       "      <td>39294.560</td>\n",
       "      <td>46416.805</td>\n",
       "      <td>32344.842</td>\n",
       "      <td>18326.2460</td>\n",
       "      <td>...</td>\n",
       "      <td>38469.840</td>\n",
       "      <td>-36466.477</td>\n",
       "      <td>51717.152</td>\n",
       "      <td>-34935.880</td>\n",
       "      <td>33479.850</td>\n",
       "      <td>-41357.324</td>\n",
       "      <td>14098.4250</td>\n",
       "      <td>13870.688</td>\n",
       "      <td>19981.975</td>\n",
       "      <td>21.897598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label        F_0        F_1        F_2        F_3        F_4        F_5  \\\n",
       "0    0.0 -3155.3340  2247.9016 -15242.063  2105.8013  4712.2056  26482.980   \n",
       "1    0.0 -4456.2400  5013.8184 -19840.195  3260.8843  5470.0503  33030.970   \n",
       "2    1.0 -3624.0283  4096.3154 -17587.424  2470.3438  5148.6680  28676.377   \n",
       "3    0.0 -4195.8760  4789.3535 -18938.963  2818.3047  5316.8325  31045.184   \n",
       "4    0.0 -5584.1300  6379.3710 -23731.717  3972.9082  6766.4370  39294.560   \n",
       "\n",
       "         F_6        F_7         F_8  ...      F_990      F_991      F_992  \\\n",
       "0  29321.460  22290.460  12380.1455  ...  24386.512 -22758.016  32603.027   \n",
       "1  37948.010  27413.734  15669.2110  ...  31678.412 -29751.850  42726.203   \n",
       "2  33115.723  23879.389  13839.6810  ...  27025.328 -26016.256  36537.504   \n",
       "3  36170.170  26332.742  15174.0280  ...  29719.297 -28442.473  40481.420   \n",
       "4  46416.805  32344.842  18326.2460  ...  38469.840 -36466.477  51717.152   \n",
       "\n",
       "       F_993      F_994      F_995       F_996      F_997      F_998  \\\n",
       "0 -22886.885  22423.560 -25667.980   7625.3670   9067.743  13116.528   \n",
       "1 -28719.370  28016.207 -33457.973  11405.8260  12074.846  16793.018   \n",
       "2 -24437.940  24452.035 -28386.310   9967.6120  10893.868  14938.218   \n",
       "3 -27291.717  26547.879 -31284.033  10983.9375  11585.030  15812.540   \n",
       "4 -34935.880  33479.850 -41357.324  14098.4250  13870.688  19981.975   \n",
       "\n",
       "        F_999  \n",
       "0  300.804350  \n",
       "1   76.287735  \n",
       "2  215.583150  \n",
       "3  209.395650  \n",
       "4   21.897598  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_output_cell_10.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8185c",
   "metadata": {},
   "source": [
    "# Beware ! Error Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17c1dc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "Cell In[21], line 18\n",
    "     15 optimizer.zero_grad()\n",
    "     17 # Forward Pass\n",
    "---> 18 outputs = model(inputs)\n",
    "     19 loss = criterion(outputs.squeeze(), labels)\n",
    "     21 # Backpropagate\n",
    "\n",
    "File ~\\Desktop\\YoloV8\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501, in Module._call_impl(self, *args, **kwargs)\n",
    "   1496 # If we don't have any hooks, we want to skip the rest of the logic in\n",
    "   1497 # this function, and just call forward.\n",
    "   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
    "   1499         or _global_backward_pre_hooks or _global_backward_hooks\n",
    "   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "-> 1501     return forward_call(*args, **kwargs)\n",
    "   1502 # Do not call functions when jit is used\n",
    "   1503 full_backward_hooks, non_full_backward_hooks = [], []\n",
    "\n",
    "Cell In[16], line 98, in TabularTransformer.forward(self, x, memory)\n",
    "     95 embedded = self.embedding(x)\n",
    "     97 print(f\" This is embeded before long:\\n {embedded}\")\n",
    "---> 98 embedded = self.embedding.weight[embedded.long(), :].to_dense()\n",
    "     99 print(f\" This is embeded after long:\\n {embedded}\")\n",
    "    100 embedded = embedded.permute(2, 0, 1)       \n",
    "\n",
    "IndexError: index -18739 is out of bounds for dimension 0 with size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c71ed",
   "metadata": {},
   "source": [
    "#### We must use scalers to rescale the data so it fits the dimention required by the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d49b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = df.columns[1:]\n",
    "\n",
    "# plt.figure()\n",
    "# fig, ax = plt.subplots(15, 2 ,figsize=(21, 36))\n",
    "\n",
    "# j = 1\n",
    "# for col in cols[0:30]:\n",
    "#     plt.subplot(15, 2, j)\n",
    "#     sns.histplot(df[col], kde=True, bins=10000, label=col, color='Red')\n",
    "#     plt.xlabel(col, fontsize=9); \n",
    "#     plt.rcParams['axes.facecolor'] = 'black'\n",
    "#     plt.legend()\n",
    "#     j += 1\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0f0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788efc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_list = []\n",
    "# max_list = []\n",
    "\n",
    "# for col in tqdm(cols):\n",
    "#     min_list.append(df[col].min())\n",
    "#     max_list.append(df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4530d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# fig, ax = plt.subplots(2, 1 ,figsize=(16, 8))\n",
    "\n",
    "# for i in range(1, 3):\n",
    "#     if i == 1:\n",
    "#         plt.subplot(2, 1, i)\n",
    "#         sns.histplot(min_list, kde=True,bins=1000, label=\"Features Min/Max Values Distribution\", color='red')\n",
    "#         plt.rcParams['axes.facecolor'] = 'black'\n",
    "#         plt.xlabel(\"Min Values\", fontsize=12, color='red')\n",
    "# #         plt.legend()\n",
    "        \n",
    "#     elif i == 2:\n",
    "#         plt.subplot(2, 1, i)\n",
    "#         sns.histplot(max_list, kde=True,bins=1000, label=\"Features Min/Max Values Distribution\", color='Green')\n",
    "#         plt.rcParams['axes.facecolor'] = 'black'\n",
    "#         plt.xlabel(\"Max Values\", fontsize=12, color='Green')\n",
    "# #         plt.legend()\n",
    "    \n",
    "#     else:\n",
    "#         break;\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7760e",
   "metadata": {},
   "source": [
    "The First approach is going to be normalizing tha values of each column to be between -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea95e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaleMe(input_value, init_min, init_max, goal_min, goal_max):\n",
    "    sclaled_value = ( ( (input_value - init_min) / (init_max - init_min) ) * (goal_max - goal_min) ) + goal_min\n",
    "    \n",
    "    return sclaled_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6a684",
   "metadata": {},
   "source": [
    "#### A simple test ...\n",
    "- Temp_vector = [1,2,3,4,5]\n",
    "- Goal_vecor = [10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d99d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp_vector = [1,2,3,4,5] # --- > [10,20,30,40,50]\n",
    "# Goal_vecor = []\n",
    "\n",
    "# init_min = min(Temp_vector)\n",
    "# init_max = max(Temp_vector)\n",
    "# goal_min = 10\n",
    "# goal_max = 50\n",
    "\n",
    "# for idx in Temp_vector:\n",
    "#     idx = ScaleMe(idx, init_min, init_max, goal_min, goal_max)\n",
    "# #     Goal_vecor.append(int(Temp_var)) \n",
    "\n",
    "# print(Temp_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefab62b",
   "metadata": {},
   "source": [
    "OK, now we are going to apply it on each column of data frame seprately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae019886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:12<00:00, 82.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>F_0</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>...</th>\n",
       "      <th>F_990</th>\n",
       "      <th>F_991</th>\n",
       "      <th>F_992</th>\n",
       "      <th>F_993</th>\n",
       "      <th>F_994</th>\n",
       "      <th>F_995</th>\n",
       "      <th>F_996</th>\n",
       "      <th>F_997</th>\n",
       "      <th>F_998</th>\n",
       "      <th>F_999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277372</td>\n",
       "      <td>-0.615168</td>\n",
       "      <td>0.224336</td>\n",
       "      <td>-0.445735</td>\n",
       "      <td>-0.269516</td>\n",
       "      <td>-0.171223</td>\n",
       "      <td>-0.222884</td>\n",
       "      <td>-0.178833</td>\n",
       "      <td>-0.181066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204326</td>\n",
       "      <td>0.254114</td>\n",
       "      <td>-0.233859</td>\n",
       "      <td>0.179850</td>\n",
       "      <td>-0.196260</td>\n",
       "      <td>0.210875</td>\n",
       "      <td>-0.377951</td>\n",
       "      <td>-0.351414</td>\n",
       "      <td>-0.256950</td>\n",
       "      <td>0.378346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.130541</td>\n",
       "      <td>0.041848</td>\n",
       "      <td>-0.134103</td>\n",
       "      <td>-0.160929</td>\n",
       "      <td>-0.059789</td>\n",
       "      <td>0.131267</td>\n",
       "      <td>0.119419</td>\n",
       "      <td>0.103986</td>\n",
       "      <td>0.134152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142357</td>\n",
       "      <td>-0.099134</td>\n",
       "      <td>0.119599</td>\n",
       "      <td>-0.128862</td>\n",
       "      <td>0.110622</td>\n",
       "      <td>-0.137260</td>\n",
       "      <td>0.095425</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>0.096342</td>\n",
       "      <td>0.105787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130408</td>\n",
       "      <td>-0.176096</td>\n",
       "      <td>0.041508</td>\n",
       "      <td>-0.355850</td>\n",
       "      <td>-0.148729</td>\n",
       "      <td>-0.069897</td>\n",
       "      <td>-0.072327</td>\n",
       "      <td>-0.091120</td>\n",
       "      <td>-0.041187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078868</td>\n",
       "      <td>0.089545</td>\n",
       "      <td>-0.096484</td>\n",
       "      <td>0.097753</td>\n",
       "      <td>-0.084953</td>\n",
       "      <td>0.089393</td>\n",
       "      <td>-0.084663</td>\n",
       "      <td>-0.110564</td>\n",
       "      <td>-0.081895</td>\n",
       "      <td>0.274889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.048901</td>\n",
       "      <td>-0.011471</td>\n",
       "      <td>-0.063849</td>\n",
       "      <td>-0.270054</td>\n",
       "      <td>-0.102191</td>\n",
       "      <td>0.039532</td>\n",
       "      <td>0.048874</td>\n",
       "      <td>0.044312</td>\n",
       "      <td>0.086695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>-0.032999</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>-0.053297</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>-0.040106</td>\n",
       "      <td>0.042598</td>\n",
       "      <td>-0.019406</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.267378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.484203</td>\n",
       "      <td>0.366222</td>\n",
       "      <td>-0.437459</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>0.298974</td>\n",
       "      <td>0.420619</td>\n",
       "      <td>0.455462</td>\n",
       "      <td>0.376196</td>\n",
       "      <td>0.388797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465246</td>\n",
       "      <td>-0.438279</td>\n",
       "      <td>0.433525</td>\n",
       "      <td>-0.457899</td>\n",
       "      <td>0.410426</td>\n",
       "      <td>-0.490281</td>\n",
       "      <td>0.432583</td>\n",
       "      <td>0.282052</td>\n",
       "      <td>0.402785</td>\n",
       "      <td>0.039758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13827</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.106008</td>\n",
       "      <td>-0.106702</td>\n",
       "      <td>-0.160814</td>\n",
       "      <td>0.058581</td>\n",
       "      <td>0.091861</td>\n",
       "      <td>0.095292</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>-0.068842</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>-0.073891</td>\n",
       "      <td>0.075323</td>\n",
       "      <td>-0.078415</td>\n",
       "      <td>0.060094</td>\n",
       "      <td>0.015908</td>\n",
       "      <td>0.073718</td>\n",
       "      <td>-0.350603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.043126</td>\n",
       "      <td>0.183206</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>-0.002382</td>\n",
       "      <td>-0.175601</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.016766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>-0.010907</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>-0.048727</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>-0.125225</td>\n",
       "      <td>-0.050151</td>\n",
       "      <td>0.306791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13829</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.440252</td>\n",
       "      <td>0.093702</td>\n",
       "      <td>-0.261933</td>\n",
       "      <td>-0.404802</td>\n",
       "      <td>0.160788</td>\n",
       "      <td>0.199729</td>\n",
       "      <td>0.250910</td>\n",
       "      <td>0.195131</td>\n",
       "      <td>0.255778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244014</td>\n",
       "      <td>-0.235995</td>\n",
       "      <td>0.204007</td>\n",
       "      <td>-0.245006</td>\n",
       "      <td>0.203682</td>\n",
       "      <td>-0.245359</td>\n",
       "      <td>0.312797</td>\n",
       "      <td>0.083168</td>\n",
       "      <td>0.192885</td>\n",
       "      <td>0.176919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13830</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483210</td>\n",
       "      <td>-0.208352</td>\n",
       "      <td>0.146442</td>\n",
       "      <td>-0.136143</td>\n",
       "      <td>-0.177333</td>\n",
       "      <td>-0.166753</td>\n",
       "      <td>-0.156148</td>\n",
       "      <td>-0.197578</td>\n",
       "      <td>-0.171186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183973</td>\n",
       "      <td>0.175960</td>\n",
       "      <td>-0.161631</td>\n",
       "      <td>0.192254</td>\n",
       "      <td>-0.166952</td>\n",
       "      <td>0.181626</td>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.098948</td>\n",
       "      <td>-0.181747</td>\n",
       "      <td>-0.214113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>-0.038983</td>\n",
       "      <td>-0.030510</td>\n",
       "      <td>-0.247598</td>\n",
       "      <td>-0.111729</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018008</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>-0.011962</td>\n",
       "      <td>-0.007253</td>\n",
       "      <td>-0.012366</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>-0.038559</td>\n",
       "      <td>-0.016745</td>\n",
       "      <td>0.177205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13832 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label       F_0       F_1       F_2       F_3       F_4       F_5  \\\n",
       "0        0.0  0.277372 -0.615168  0.224336 -0.445735 -0.269516 -0.171223   \n",
       "1        0.0 -0.130541  0.041848 -0.134103 -0.160929 -0.059789  0.131267   \n",
       "2        1.0  0.130408 -0.176096  0.041508 -0.355850 -0.148729 -0.069897   \n",
       "3        0.0 -0.048901 -0.011471 -0.063849 -0.270054 -0.102191  0.039532   \n",
       "4        0.0 -0.484203  0.366222 -0.437459  0.014633  0.298974  0.420619   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "13827    0.0 -0.028082 -0.106008 -0.106702 -0.160814  0.058581  0.091861   \n",
       "13828    0.0 -0.043126  0.183206  0.004159 -0.002382 -0.175601  0.004300   \n",
       "13829    0.0 -0.440252  0.093702 -0.261933 -0.404802  0.160788  0.199729   \n",
       "13830    0.0  0.483210 -0.208352  0.146442 -0.136143 -0.177333 -0.166753   \n",
       "13831    0.0  0.003372 -0.038983 -0.030510 -0.247598 -0.111729  0.005667   \n",
       "\n",
       "            F_6       F_7       F_8  ...     F_990     F_991     F_992  \\\n",
       "0     -0.222884 -0.178833 -0.181066  ... -0.204326  0.254114 -0.233859   \n",
       "1      0.119419  0.103986  0.134152  ...  0.142357 -0.099134  0.119599   \n",
       "2     -0.072327 -0.091120 -0.041187  ... -0.078868  0.089545 -0.096484   \n",
       "3      0.048874  0.044312  0.086695  ...  0.049213 -0.032999  0.041221   \n",
       "4      0.455462  0.376196  0.388797  ...  0.465246 -0.438279  0.433525   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "13827  0.095292  0.073248  0.085436  ...  0.104352 -0.068842  0.077622   \n",
       "13828  0.033599 -0.003198 -0.016766  ...  0.043652 -0.002709  0.016395   \n",
       "13829  0.250910  0.195131  0.255778  ...  0.244014 -0.235995  0.204007   \n",
       "13830 -0.156148 -0.197578 -0.171186  ... -0.183973  0.175960 -0.161631   \n",
       "13831  0.016766  0.002611  0.035085  ...  0.018008 -0.002359  0.013903   \n",
       "\n",
       "          F_993     F_994     F_995     F_996     F_997     F_998     F_999  \n",
       "0      0.179850 -0.196260  0.210875 -0.377951 -0.351414 -0.256950  0.378346  \n",
       "1     -0.128862  0.110622 -0.137260  0.095425  0.045196  0.096342  0.105787  \n",
       "2      0.097753 -0.084953  0.089393 -0.084663 -0.110564 -0.081895  0.274889  \n",
       "3     -0.053297  0.030051 -0.040106  0.042598 -0.019406  0.002123  0.267378  \n",
       "4     -0.457899  0.410426 -0.490281  0.432583  0.282052  0.402785  0.039758  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "13827 -0.073891  0.075323 -0.078415  0.060094  0.015908  0.073718 -0.350603  \n",
       "13828 -0.010907  0.002114 -0.048727  0.036789 -0.125225 -0.050151  0.306791  \n",
       "13829 -0.245006  0.203682 -0.245359  0.312797  0.083168  0.192885  0.176919  \n",
       "13830  0.192254 -0.166952  0.181626 -0.148129 -0.098948 -0.181747 -0.214113  \n",
       "13831 -0.011962 -0.007253 -0.012366  0.004602 -0.038559 -0.016745  0.177205  \n",
       "\n",
       "[13832 rows x 1001 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_min = df.iloc[:,1:].min()\n",
    "init_max = df.iloc[:,1:].max()\n",
    "\n",
    "goal_min = -1\n",
    "goal_max = 1\n",
    "\n",
    "# Apply the ScaleMe function to each column\n",
    "scaled_df = pd.DataFrame()\n",
    "for column in tqdm(df.iloc[:,1:].columns):\n",
    "    df[column] = ScaleMe(df[column], init_min[column], init_max[column], goal_min, goal_max)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd7d03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6161a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13832 entries, 0 to 13831\n",
      "Columns: 1001 entries, Label to F_999\n",
      "dtypes: float64(1000), int32(1)\n",
      "memory usage: 105.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98b75c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>F_0</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>...</th>\n",
       "      <th>F_990</th>\n",
       "      <th>F_991</th>\n",
       "      <th>F_992</th>\n",
       "      <th>F_993</th>\n",
       "      <th>F_994</th>\n",
       "      <th>F_995</th>\n",
       "      <th>F_996</th>\n",
       "      <th>F_997</th>\n",
       "      <th>F_998</th>\n",
       "      <th>F_999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "      <td>13832.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.091888</td>\n",
       "      <td>0.050002</td>\n",
       "      <td>-0.146251</td>\n",
       "      <td>-0.023052</td>\n",
       "      <td>-0.234028</td>\n",
       "      <td>-0.058076</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.014331</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.009463</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>-0.015677</td>\n",
       "      <td>-0.011731</td>\n",
       "      <td>-0.042348</td>\n",
       "      <td>-0.006613</td>\n",
       "      <td>0.030028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288879</td>\n",
       "      <td>0.380545</td>\n",
       "      <td>0.396829</td>\n",
       "      <td>0.329379</td>\n",
       "      <td>0.288576</td>\n",
       "      <td>0.303694</td>\n",
       "      <td>0.312743</td>\n",
       "      <td>0.332670</td>\n",
       "      <td>0.304844</td>\n",
       "      <td>0.307483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330350</td>\n",
       "      <td>0.328886</td>\n",
       "      <td>0.325247</td>\n",
       "      <td>0.323179</td>\n",
       "      <td>0.319912</td>\n",
       "      <td>0.330032</td>\n",
       "      <td>0.369707</td>\n",
       "      <td>0.318389</td>\n",
       "      <td>0.328246</td>\n",
       "      <td>0.245453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.222134</td>\n",
       "      <td>-0.497614</td>\n",
       "      <td>-0.262873</td>\n",
       "      <td>-0.462236</td>\n",
       "      <td>-0.259246</td>\n",
       "      <td>-0.209042</td>\n",
       "      <td>-0.234745</td>\n",
       "      <td>-0.222203</td>\n",
       "      <td>-0.200742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227611</td>\n",
       "      <td>-0.237108</td>\n",
       "      <td>-0.241835</td>\n",
       "      <td>-0.247751</td>\n",
       "      <td>-0.225891</td>\n",
       "      <td>-0.258149</td>\n",
       "      <td>-0.312477</td>\n",
       "      <td>-0.262782</td>\n",
       "      <td>-0.243810</td>\n",
       "      <td>-0.124988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020511</td>\n",
       "      <td>-0.061110</td>\n",
       "      <td>-0.051866</td>\n",
       "      <td>-0.257789</td>\n",
       "      <td>-0.036339</td>\n",
       "      <td>0.027929</td>\n",
       "      <td>0.044811</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.040671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>-0.023309</td>\n",
       "      <td>0.026419</td>\n",
       "      <td>-0.027280</td>\n",
       "      <td>0.021489</td>\n",
       "      <td>-0.034050</td>\n",
       "      <td>0.047533</td>\n",
       "      <td>-0.019619</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.028527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374754</td>\n",
       "      <td>0.148974</td>\n",
       "      <td>0.213304</td>\n",
       "      <td>-0.050767</td>\n",
       "      <td>0.145703</td>\n",
       "      <td>0.237206</td>\n",
       "      <td>0.257521</td>\n",
       "      <td>0.210005</td>\n",
       "      <td>0.236339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258834</td>\n",
       "      <td>0.250943</td>\n",
       "      <td>0.237142</td>\n",
       "      <td>0.222506</td>\n",
       "      <td>0.233489</td>\n",
       "      <td>0.229759</td>\n",
       "      <td>0.263455</td>\n",
       "      <td>0.169665</td>\n",
       "      <td>0.228592</td>\n",
       "      <td>0.200233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Label           F_0           F_1           F_2           F_3  \\\n",
       "count  13832.000000  13832.000000  13832.000000  13832.000000  13832.000000   \n",
       "mean       0.091888      0.050002     -0.146251     -0.023052     -0.234028   \n",
       "std        0.288879      0.380545      0.396829      0.329379      0.288576   \n",
       "min        0.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%        0.000000     -0.222134     -0.497614     -0.262873     -0.462236   \n",
       "50%        0.000000      0.020511     -0.061110     -0.051866     -0.257789   \n",
       "75%        0.000000      0.374754      0.148974      0.213304     -0.050767   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                F_4           F_5           F_6           F_7           F_8  \\\n",
       "count  13832.000000  13832.000000  13832.000000  13832.000000  13832.000000   \n",
       "mean      -0.058076      0.011126      0.014331     -0.010526      0.012198   \n",
       "std        0.303694      0.312743      0.332670      0.304844      0.307483   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.259246     -0.209042     -0.234745     -0.222203     -0.200742   \n",
       "50%       -0.036339      0.027929      0.044811      0.009569      0.040671   \n",
       "75%        0.145703      0.237206      0.257521      0.210005      0.236339   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...         F_990         F_991         F_992         F_993  \\\n",
       "count  ...  13832.000000  13832.000000  13832.000000  13832.000000   \n",
       "mean   ...      0.015931      0.004815     -0.000148     -0.009463   \n",
       "std    ...      0.330350      0.328886      0.325247      0.323179   \n",
       "min    ...     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%    ...     -0.227611     -0.237108     -0.241835     -0.247751   \n",
       "50%    ...      0.043060     -0.023309      0.026419     -0.027280   \n",
       "75%    ...      0.258834      0.250943      0.237142      0.222506   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              F_994         F_995         F_996         F_997         F_998  \\\n",
       "count  13832.000000  13832.000000  13832.000000  13832.000000  13832.000000   \n",
       "mean       0.002045     -0.015677     -0.011731     -0.042348     -0.006613   \n",
       "std        0.319912      0.330032      0.369707      0.318389      0.328246   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.225891     -0.258149     -0.312477     -0.262782     -0.243810   \n",
       "50%        0.021489     -0.034050      0.047533     -0.019619      0.014742   \n",
       "75%        0.233489      0.229759      0.263455      0.169665      0.228592   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              F_999  \n",
       "count  13832.000000  \n",
       "mean       0.030028  \n",
       "std        0.245453  \n",
       "min       -1.000000  \n",
       "25%       -0.124988  \n",
       "50%        0.028527  \n",
       "75%        0.200233  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 1001 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd60fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>F_0</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>...</th>\n",
       "      <th>F_990</th>\n",
       "      <th>F_991</th>\n",
       "      <th>F_992</th>\n",
       "      <th>F_993</th>\n",
       "      <th>F_994</th>\n",
       "      <th>F_995</th>\n",
       "      <th>F_996</th>\n",
       "      <th>F_997</th>\n",
       "      <th>F_998</th>\n",
       "      <th>F_999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13827</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.106008</td>\n",
       "      <td>-0.106702</td>\n",
       "      <td>-0.160814</td>\n",
       "      <td>0.058581</td>\n",
       "      <td>0.091861</td>\n",
       "      <td>0.095292</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>-0.068842</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>-0.073891</td>\n",
       "      <td>0.075323</td>\n",
       "      <td>-0.078415</td>\n",
       "      <td>0.060094</td>\n",
       "      <td>0.015908</td>\n",
       "      <td>0.073718</td>\n",
       "      <td>-0.350603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13828</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.043126</td>\n",
       "      <td>0.183206</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>-0.002382</td>\n",
       "      <td>-0.175601</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.016766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>-0.010907</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>-0.048727</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>-0.125225</td>\n",
       "      <td>-0.050151</td>\n",
       "      <td>0.306791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13829</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.440252</td>\n",
       "      <td>0.093702</td>\n",
       "      <td>-0.261933</td>\n",
       "      <td>-0.404802</td>\n",
       "      <td>0.160788</td>\n",
       "      <td>0.199729</td>\n",
       "      <td>0.250910</td>\n",
       "      <td>0.195131</td>\n",
       "      <td>0.255778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244014</td>\n",
       "      <td>-0.235995</td>\n",
       "      <td>0.204007</td>\n",
       "      <td>-0.245006</td>\n",
       "      <td>0.203682</td>\n",
       "      <td>-0.245359</td>\n",
       "      <td>0.312797</td>\n",
       "      <td>0.083168</td>\n",
       "      <td>0.192885</td>\n",
       "      <td>0.176919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13830</th>\n",
       "      <td>0</td>\n",
       "      <td>0.483210</td>\n",
       "      <td>-0.208352</td>\n",
       "      <td>0.146442</td>\n",
       "      <td>-0.136143</td>\n",
       "      <td>-0.177333</td>\n",
       "      <td>-0.166753</td>\n",
       "      <td>-0.156148</td>\n",
       "      <td>-0.197578</td>\n",
       "      <td>-0.171186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183973</td>\n",
       "      <td>0.175960</td>\n",
       "      <td>-0.161631</td>\n",
       "      <td>0.192254</td>\n",
       "      <td>-0.166952</td>\n",
       "      <td>0.181626</td>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.098948</td>\n",
       "      <td>-0.181747</td>\n",
       "      <td>-0.214113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>-0.038983</td>\n",
       "      <td>-0.030510</td>\n",
       "      <td>-0.247598</td>\n",
       "      <td>-0.111729</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018008</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>-0.011962</td>\n",
       "      <td>-0.007253</td>\n",
       "      <td>-0.012366</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>-0.038559</td>\n",
       "      <td>-0.016745</td>\n",
       "      <td>0.177205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label       F_0       F_1       F_2       F_3       F_4       F_5  \\\n",
       "13827      0 -0.028082 -0.106008 -0.106702 -0.160814  0.058581  0.091861   \n",
       "13828      0 -0.043126  0.183206  0.004159 -0.002382 -0.175601  0.004300   \n",
       "13829      0 -0.440252  0.093702 -0.261933 -0.404802  0.160788  0.199729   \n",
       "13830      0  0.483210 -0.208352  0.146442 -0.136143 -0.177333 -0.166753   \n",
       "13831      0  0.003372 -0.038983 -0.030510 -0.247598 -0.111729  0.005667   \n",
       "\n",
       "            F_6       F_7       F_8  ...     F_990     F_991     F_992  \\\n",
       "13827  0.095292  0.073248  0.085436  ...  0.104352 -0.068842  0.077622   \n",
       "13828  0.033599 -0.003198 -0.016766  ...  0.043652 -0.002709  0.016395   \n",
       "13829  0.250910  0.195131  0.255778  ...  0.244014 -0.235995  0.204007   \n",
       "13830 -0.156148 -0.197578 -0.171186  ... -0.183973  0.175960 -0.161631   \n",
       "13831  0.016766  0.002611  0.035085  ...  0.018008 -0.002359  0.013903   \n",
       "\n",
       "          F_993     F_994     F_995     F_996     F_997     F_998     F_999  \n",
       "13827 -0.073891  0.075323 -0.078415  0.060094  0.015908  0.073718 -0.350603  \n",
       "13828 -0.010907  0.002114 -0.048727  0.036789 -0.125225 -0.050151  0.306791  \n",
       "13829 -0.245006  0.203682 -0.245359  0.312797  0.083168  0.192885  0.176919  \n",
       "13830  0.192254 -0.166952  0.181626 -0.148129 -0.098948 -0.181747 -0.214113  \n",
       "13831 -0.011962 -0.007253 -0.012366  0.004602 -0.038559 -0.016745  0.177205  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25877",
   "metadata": {},
   "source": [
    "Now lets see what its looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5a5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del min_list, max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e283fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_list = []\n",
    "# max_list = []\n",
    "# cols = df.columns[1:]\n",
    "\n",
    "# for col in tqdm(cols):\n",
    "#     min_list.append(df[col].min())\n",
    "#     max_list.append(df[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b031287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = df.columns[1:]\n",
    "\n",
    "# plt.figure()\n",
    "# fig, ax = plt.subplots(15, 2 ,figsize=(21, 36))\n",
    "\n",
    "# j = 1\n",
    "# for col in cols[0:30]:\n",
    "#     plt.subplot(15, 2, j)\n",
    "#     sns.histplot(df[col], kde=True, bins=1000, label=col, color='Red')\n",
    "#     plt.xlabel(col, fontsize=9); \n",
    "#     plt.rcParams['axes.facecolor'] = 'black'\n",
    "#     plt.legend()\n",
    "#     j += 1\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013983e",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db62fce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df['Label'], test_size=0.2, random_state=42, stratify=df['Label'])\n",
    "\n",
    "# Delete the residual\n",
    "del df \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b44644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them as PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6af8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for batching the data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e6bf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for batching the data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040e516",
   "metadata": {},
   "source": [
    "# Define the Tabular Transformer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cbe0a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,                   # Total number of Feature columns\n",
    "        hidden_dim,                  # Neuron Density of MLP Layers\n",
    "        output_dim,                  # Total number of Label columns\n",
    "        num_attention_heads,         # Total Number of attention heads of Transformer\n",
    "        num_transformer_layers,      # Total Number of transformer layers\n",
    "        num_mlp_layers,              # Total number of MLP layers\n",
    "        middle_activation,           # MLP layer Activation Function\n",
    "        last_activation,             # Last layer Activation Function\n",
    "        dropout_prob,                # Dropout possibility of Dropout layers (float)\n",
    "        pooling_type                 # Pooling Strategy : mean(Average) , max(Maximum), global(mean)\n",
    "    ):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        \n",
    "        # New Line\n",
    "        self.pooling_type = pooling_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"Should I remove this because of the situation with the data in the next steps\"\"\"\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model = hidden_dim,\n",
    "                nhead = num_attention_heads,\n",
    "            ),\n",
    "            num_layers = num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # MLP Layers\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(num_mlp_layers):\n",
    "            # Last FC Layer\n",
    "            if i == num_mlp_layers-1:\n",
    "                self.mlp.add_module(\n",
    "                    f\"FC_{i+1}\",\n",
    "                    nn.Linear(hidden_dim, output_dim)\n",
    "                )\n",
    "                   \n",
    "                \n",
    "                ######## Suspicion Area ##########\n",
    "            else:\n",
    "                self.mlp.add_module(\n",
    "                    f\"FC_{i+1}\",\n",
    "                    nn.Linear(hidden_dim,hidden_dim)\n",
    "                )    \n",
    "                ######## Suspicion Area ##########\n",
    "                \n",
    "                \n",
    "            # Activation Functions for Final FC Layer\n",
    "            if i == num_mlp_layers-1:\n",
    "                if last_activation == \"sigmoid\":\n",
    "                    self.mlp.add_module(\n",
    "                        f\"Activation_{i+1}\",\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "                elif last_activation == \"softmax\":\n",
    "                    self.mlp.add_module(\n",
    "                        f\"Activation_{i+1}\",\n",
    "                        nn.Softmax(dim=-1)\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid activation function for the last layer: {last_activation}\")\n",
    "            \n",
    "            # Activation Functions for other FC layers\n",
    "            else:\n",
    "                if middle_activation == \"relu\":\n",
    "                    self.mlp.add_module(\n",
    "                        f\"Activation_{i+1}\",\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "                elif middle_activation == \"leakyrelu\":\n",
    "                    self.mlp.add_module(\n",
    "                        f\"Activation_{i+1}\",\n",
    "                        nn.LeakyReLU()\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid activation function for the middle layers : {last_activation} \")\n",
    "            \n",
    "            # Dropout Layer\n",
    "            if i != num_mlp_layers-1:\n",
    "                if dropout_prob > 0.0:\n",
    "                    self.mlp.add_module(\n",
    "                        f\"Dropout_{i+1}\",\n",
    "                        nn.Dropout(dropout_prob)\n",
    "                    )\n",
    "       \n",
    "                    \n",
    "    def forward(self,  x, memory=None): \n",
    "        \n",
    "        print(x.shape)\n",
    "        # Apply linear transformation\n",
    "        embedded = self.embedding(x)    \n",
    "        \"\"\"This line seems meaningless to me\"\"\"\n",
    "#         embedded = self.embedding.weight[embedded.long(), :].to_dense()\n",
    "        print(embedded.shape)\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "        print(embedded.shape)\n",
    "        embedded = embedded.permute(1, 0, 2)   \n",
    "#         memory = torch.rand(input_dim, 1, hidden_dim).to(device)\n",
    "        memory = torch.rand(self.input_dim, x.size(0), self.hidden_dim).to(device)\n",
    "        transformer_output = self.transformer(embedded, memory)\n",
    "    \n",
    "#         # Reset the dimensions to original order for upcoming FC/Pooling Layers\n",
    "        transformer_output = transformer_output.permute(1, 0, 2)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        if self.pooling_type == \"mean\":\n",
    "            pooled_output = torch.mean(transformer_output, dim=1)\n",
    "        elif self.pooling_type == \"max\":\n",
    "            pooled_output = torch.max(transformer_output, dim=1) \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid pooling_type: {self.pooling_type}\")\n",
    "        \n",
    "        pooled_output = pooled_output.permute(1,0)\n",
    "        mlp_output = self.mlp(pooled_output) \n",
    "        \n",
    "\n",
    "        return mlp_output\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1276056",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4350549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2d8d5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularTransformer(\n",
    "    input_dim = 1000,\n",
    "    hidden_dim = 128,\n",
    "    output_dim = 1,\n",
    "    num_attention_heads = 8,\n",
    "    num_transformer_layers = 2,\n",
    "    num_mlp_layers = 4,\n",
    "    middle_activation = 'relu',\n",
    "    last_activation = 'sigmoid',\n",
    "    dropout_prob = 0.1,\n",
    "    pooling_type = 'mean'\n",
    "\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "720ab12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "efcef0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m temp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1000\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# mlp_output = model(temp)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Apply threshold (0.5) to convert probabilities to binary values\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# binary_output = (mlp_output >= 0.5).float()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# binary_output\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\YoloV8\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[123], line 100\u001b[0m, in \u001b[0;36mTabularTransformer.forward\u001b[1;34m(self, x, memory)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Apply linear transformation\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m         embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"This line seems meaningless to me\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m#         embedded = self.embedding.weight[embedded.long(), :].to_dense()\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\YoloV8\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\YoloV8\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\YoloV8\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "temp = torch.rand(8, 1000, dtype=torch.float32).to(device)\n",
    "model(temp)\n",
    "# mlp_output = model(temp)\n",
    "# # Apply threshold (0.5) to convert probabilities to binary values\n",
    "# binary_output = (mlp_output >= 0.5).float()\n",
    "# binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c40d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "model.train()\n",
    "\n",
    "init_outputs_list = []\n",
    "final_outputs_list = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"Epoch {epoch} in progress ...\\n\")\n",
    "#     running_loss = 0.0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # All set to work on same device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Reset the Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights and loss\n",
    "        optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "\n",
    "    \"\"\"Evaluation\"\"\"\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            \n",
    "            test_outputs = model(test_inputs)\n",
    "            binary_predictions = (test_outputs >= 0.5).float()\n",
    "            \n",
    "            test_labels = test_labels.squeeze()\n",
    "            val_loss = criterion(test_outputs.squeeze(), test_labels)\n",
    "            \n",
    "            # Calculate the number of true positives, false negatives, and false positives\n",
    "            TP = ((binary_predictions == 1.0) & (test_labels == 1.0)).sum().item()\n",
    "            FN = ((binary_predictions == 0.0) & (test_labels == 1.0)).sum().item()\n",
    "            FP = ((binary_predictions == 1.0) & (test_labels == 0.0)).sum().item()\n",
    "            \n",
    "            loss = criterion(binary_predictions.squeeze(), test_labels.squeeze())\n",
    "        \n",
    "        \n",
    "        # Metric Calculations\n",
    "        if TP == 0 and FN == 0:  # (Recall is 0/0)\n",
    "            recall = 0.0\n",
    "            precision = 0.0\n",
    "            f1_score = 0.0\n",
    "\n",
    "        elif TP == 0 and FP == 0:  # (Precision is 0/0)\n",
    "            recall = 0.0\n",
    "            precision = 0.0\n",
    "            f1_score = 0.0\n",
    "\n",
    "        else:\n",
    "            recall = TP/(TP+FN)\n",
    "            precision = TP/(TP+FP)\n",
    "            f1_score = (2*precision*recall)/(precision+recall)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {val_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e7982",
   "metadata": {},
   "source": [
    "# Checking the output values Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del init_outputs_list, final_outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final_outputs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# fig, ax = plt.subplots(1, 1 ,figsize=(16, 5))\n",
    "\n",
    "# for i in range(1, 2):\n",
    "#     if i == 1:\n",
    "#         plt.subplot(1, 1, i)\n",
    "#         sns.histplot(init_outputs_list, kde=True,bins=11065, label=\"Model Output Values\", color='red')\n",
    "#         plt.rcParams['axes.facecolor'] = 'black'\n",
    "#         plt.xlabel(\"Model 1st epoch Output Values\", fontsize=12, color='red')\n",
    "    \n",
    "#     else:\n",
    "#         break;\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plotting the first list\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.histplot(init_outputs_list, kde=True, bins=11065, label=\"Model Output Values\", color='red')\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.xlabel(\"Model 1st epoch Output Values\", fontsize=12, color='red')\n",
    "\n",
    "# Plotting the second list\n",
    "plt.hist(final_outputs_list, kde=True, bins=11065, label=\"Second List Values\", color='blue')\n",
    "plt.xlabel(\"Second List Values\", fontsize=12, color='blue')\n",
    "\n",
    "plt.legend()  # Show legend for both plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf95d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
